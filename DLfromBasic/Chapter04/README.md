# 제 4고지. 신경망 만들기
## step 37: 텐서를 다루다
머신러닝에서는 텐서 단위의 입력값을 주로 다루기에 dezero는 텐서를 사용하는 연산을 수행할 수 있어야 한다.  
dezero는 ndarray를 활용하기 때문에 numpy의 원소별 연산과 브로드캐스팅 기능이 자동적으로 적용되어 있어, 순전파와 역전파가 추가적인 구현 없이 가능하다.

자동미분에서 거꾸로 가는 방향(역전파)을 채택한 이유는 텐서의 계산에서 연산 효율을 높이기 위해서이다.

## step 37: 텐서를 다루다

## step 37: 텐서를 다루다

## step 41: 행렬의 곱
p336. dL/dxi는 xi를 (미세하게) 변화시켰을 때 L이 얼마나 변화하느냐를 뜻하는 '변화율'을 말합니다. 여기서 xi를 변화시키면 벡터 y의 모든 원소가 변화합니다. 그리고 y의 각 원소의 변화를 통해 궁극적으로 L이 변화하게 됩니다. 따라서 xi에서 L에 이르는 연쇄 법칙의 경로는 여러개 있고, 그 총합이 dL/dxi 입니다.

## step 42: 선형 회귀
p342. x로부터 실숫값 y를 예측하는 것을 '회귀(regression)'라고 합니다. 그리고 회귀 모델 중 예측값이 선형(직선)을 이루는 것을 '선형 회귀'라고 합니다.

p343. 우리의 목표는 데이터에 맞는 직선 y = Wx+b를 찾는 것입니다. 그러기 위해서는 데이터와 예측치의 차이, 즉 '잔차(residual)'를 최소화해야 합니다.

p343. L = 1/N*sum(f(xi)-yi)^2 은 총 N개의 점에 대해 (xi, yi)의 각 점에서 제곱오차를 구한 다음 모두 더합니다. 그리고 평균을 구하기 위해 1/N을 곱합니다. 이 수식을 평균 제곱 오차(mean squared error)라고 합니다.

p343. 모델의 성능이 얼마나 '나쁜가'를 평가하는 함수를 손실 함수라고 합니다. 따라서 선형 회귀는 '손실 함수로 평균 제곱 오차를 이용한다'고 말할 수 있습니다.

p343. 우리의 목표는 손실 함수의 출력을 최소화하는 W와 b를 찾은ㄴ 것입니다. 기억하시겠지만, 바로 함수 최적화 문제입니다.

## step 43: 신경망
p351. y = F.matmul(x, W) + b 이와 같이 입력 x와 매개변수 W 사이에서 행렬 곱을 구하고, 거기에 b를 더합니다. 이 변환을 선형 변환(linear transformation) 혹은 아핀 변환(affine transformation)이라고 합니다.
선형 변환은 엄밀히 말하면 y = F.matmul(x,W) 까지로, b는 포함되지 않습니다. 그러나 신경망 분야에서는 b를 더하는 계산까지 포함하는 연산을 선형 변환이라고 부르는 것이 일반적입니다. 그리고 선형 변환은 신경망에서는 완전연결계층(fully connected layer)에 해당하며, 매개변수 W는 가중치(weight), 매개변수 b는 편향(bias)라고 합니다.

p355. 선형 변환은 이름 그대로 입력 데이터를 선형으로 변환해줍니다. 한편 신경망은 선형 변환의 출력에 비선형 변환을 수행합니다. 이 비선형 변환을 활성화 함수(activation function)라고 하며, 대표적으로 ReLU 함수와 시그모이드 함수(sigmoid function)등이 있습니다. 

p356. 신경망에서는 선형 변환이나 활성화 함수 등에 의한 변환을 층(layer)이라고 합니다. 또한 선형 변환과 같이 매개변수가 있는 층 N개가 연속으로 이어져 N번의 변환을 수행하는 구조를 'N층 신경망'이라고 합니다.

## step 50: 미니배치를 뽑아주는 DataLoader
p412. for문에서 리스트의 원소를 꺼낼 때 내부적으로(사용자에게 보이지 않는 곳에서) 반복자가 이용됩니다. 예를 들어 t=[1,2,3]일 때 for x in t:x를 실행하면 리스트 t가 내부적으로 반복자로 변환됩니다.

p414. 'Dataset 인터페이스를 만족하는 인스턴스'란 __getitem__ 과 __len__ 메서드를 구현한 클래스로부터 생성된 인스턴스를 말합니다.

p419. 과대적합ovrefitting은 특정 훈련 데이터에 지나치게 최적화된 상태를 말합니다. 따라서 새로운 데이터에서는 예측 정확도가 훨씬 떨어지는, 달리 표현하면 일반화되지 못한 상태를 뜻합니다. 신경망으로는 표현력이 높은 모델을 만들 수 있기 때문에 과대적합이 흔히 일어납니다.

## step 51: MNIST 학습